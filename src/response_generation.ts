import { PromptTemplate } from "@langchain/core/prompts";
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";
import { createClient } from "@supabase/supabase-js";
import { SupabaseVectorStore } from "@langchain/community/vectorstores/supabase";
import { Document } from "@langchain/core/documents";
import { GoogleGenerativeAIEmbeddings } from "@langchain/google-genai";
import {
  RunnableSequence,
  RunnablePassthrough,
} from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";
require("dotenv").config();

/**
 * We use the standalone questino to retrieve the most relevant documents from the vector store
 * But while generating the response from the llm we use the original user question, as it may contain some data which can be used by the llm to generate better responses
 */

const geminiApiKey = process.env.GEMINI_API_KEY;
const supabaseUrl = process.env.SUPABASE_URL as string;
const supabaseKey = process.env.SUPABASE_API_KEY as string;

const embeddings = new GoogleGenerativeAIEmbeddings({
  apiKey: geminiApiKey,
  model: "text-embedding-004",
});
const client = createClient(supabaseUrl, supabaseKey);
const llm = new ChatGoogleGenerativeAI({
  apiKey: geminiApiKey,
  model: "gemini-1.5-flash",
  maxRetries: 2,
});

const convHistory: string[] = [];

const vectorStore = new SupabaseVectorStore(embeddings, {
  client,
  tableName: "documents", // Name of the table in supabase where the documents are stored
  queryName: "match_documents", // Name of the function in supabase used to match the documents
});

const retriever = vectorStore.asRetriever();

function generateStandaloneQuestionPrompt() {
  const standaloneQuestion =
    "Given a user question, generate a suitable standalone question from it. The user question is: {question} .\n The previous conversation history with the user is {convHistory}, Make sure to use the context from the conversation history to generate a suitable standalone question";
  const standalonePrompt = PromptTemplate.fromTemplate(standaloneQuestion);
  return standalonePrompt;
}

function combineDocuments(documents: Document[]) {
  return documents.map((doc) => doc.pageContent).join("\n\n");
}

function retrieveDocumentsFromVectorStore() {
  return (
    generateStandaloneQuestionPrompt()
      .pipe(llm)
      .pipe((output) => output.content) // Extracting the standalone question generated by the llm and passing it to the retriever
      //! Instead of this even StringOutputParser can be used like .pipe(new StringOutputParser()), output parser are used to generate the output in a specific format
      .pipe(retriever)
      .pipe(combineDocuments)
  );
}

function generateResponsePrompt() {
  const answerTemplate = `
        You are a helpful and enthusiastic support bot for helping students with system design questions based on the context provided and previous conversation history with the student. Make sure to use the conversation history along with the context to generate the answer. Try to find the answer in the context. If you really don't know the answer, say "I'm sorry, I don't know the answer to that". Don't try to make up the answer. Always speak as if you were speaking to a friend.
        The previous conversation history with the user is {convHistory}
        context : {context}
        question : {question}
        answer : 
    `;
  return PromptTemplate.fromTemplate(answerTemplate)
    .pipe(llm)
    .pipe(new StringOutputParser());
}

function createPipeline() {
  return RunnableSequence.from([
    {
      originalInput: new RunnablePassthrough(),
    },
    {
      question: ({ originalInput }) => originalInput.question,
      convHistory: ({ originalInput }) => originalInput.convHistory,
    },
    {
      context: retrieveDocumentsFromVectorStore(),
      //* Here we can use Something like context : prevResult => prevResult.question to access the output of the previous operation
      //! Instead of using pipes we can also use nested RunnableSequence to create a pipeline
      //! RunnableSequence is used instead of pipe to create a pipeline so that input from the first operation can be available until the last operation
      question: ({ question }) => question,
      convHistory: ({ convHistory }) => convHistory,
    },
    generateResponsePrompt(),
  ]);
}

function formatConvHistory(messages: string[]) {
  return messages
    .map((message, index) => {
      if (index % 2 === 0) {
        return `Human: ${message}`;
      } else {
        return `AI: ${message}`;
      }
    })
    .join("\n");
}

async function generateResponse(question: string) {
  //   const chain = retrieveDocumentsFromVectorStore().pipe(combineDocuments);
  //   const response = await chain.invoke({ question });
  //   console.log(response);

  const pipeline = createPipeline();
  const response = await pipeline.invoke({
    question,
    convHistory: formatConvHistory(convHistory),
  });
  convHistory.push(question);
  convHistory.push(response);
  console.log(response);
}

generateResponse("What is CDN?");

// async function generateResponse(question: string) {}

// generateResponse("What is the capital of France?");
